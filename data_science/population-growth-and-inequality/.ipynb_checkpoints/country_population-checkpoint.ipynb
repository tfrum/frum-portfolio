{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb713f1-1ac4-4d7e-b988-846ce93acd0f",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This project is a structured walk through of cleaning, prep, and analysis of a dataset sourced from Gapminder. The nature of the data is not typical business data, and isntead reflects my background as a social scientist, being historical information (population by country) matched against a socio-political index (GINI coeffcient, a measurement of inequality.) The insights drawn from it are therefore social scientific in nature and some of the methodology I use is particular to the social sciences--a fact which we'll discuss as we get into prepping the dataset for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abe5980-73bd-4aac-8b1b-e04e9a02abb1",
   "metadata": {},
   "source": [
    "# Import - Initial Data Frame\n",
    "\n",
    "Here we import the ddf-gapminder--system_globalis dataset and a series of tools to treat it. \n",
    "This dataset a large set of population trend data with an extremely wide range of dates and projections. \n",
    "We also set some formating options to make the presentation of that data better in a jupyter notebook like this.\n",
    "\n",
    "Once imported the data is in long format, and we need to put it into wide format to carry out our analysis. It isn't necessary (though it is easier), but it's more readable with this category of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a48d180-1c34-466d-b18e-72135785c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbn\n",
    "import matplotlib.pyplot as mpl\n",
    "\n",
    "# this code is meant only to improve readability, it autoformats floats to have zero significant digits in certain contexts.\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "\n",
    "\n",
    "# This code imports and then pivots a subset of the auto-generated ddf-gapminder--system_globalis set.\n",
    "df = pd.read_csv('ddf--datapoints--total_population_with_projections--by--geo--time.csv')\n",
    "df = df.pivot_table(index='geo', columns='time', values='total_population_with_projections').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a74ca8-d514-4481-8ab3-f929f9ada1f7",
   "metadata": {},
   "source": [
    "# Import and Filter - Additional Dataframe\n",
    "\n",
    "Our initial dataset gives us 252 countries with population data from 1086 to 2100, including projections past 2013 at 2050 and 2100.\n",
    "But wait! There aren't 252 countries. So it looks like we have non-country information in this list. I want to just work with countries, so what do we do?\n",
    "Simple, we just need to see where they intersect with the iso3 country codes list.\n",
    "\n",
    "To do that we'll import a list of country iso codes and, after ensuring our dataframe's own iso3 codes are in the same case, do an intersect operation.\n",
    "\n",
    "Commented our below is an example of doing this without using a vecotrized operation, which is far slower. \n",
    "It should be noted that whether or not to handle small n data with vectorized operations can often come down to readbility. While the loop is 29x slower, it can be more readonable in certain circumstances. In this case neither is superior, so we use the vectorized operation.\n",
    "\n",
    "> The vectorized operation took 0.00045 seconds while the loop took 0.013 seconds. 29x faster.\n",
    "\n",
    "Below we should see that our data is now much more restricted. Now, you may notice that it still has more entries than there are independent countries. Why is that? Well, it includes subnational entities, such as British Crown dependencies and Chinese special administrative regions. This is fine, as these regions are generally distinct from their parent polity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ebc5f70-9a66-4fb3-ad3b-c1d35478d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and clean filter dataset\n",
    "\n",
    "df_country_code = pd.read_csv('country_iso_codes.csv')\n",
    "\n",
    "df['geo'] = df['geo'].str.upper()\n",
    "\n",
    "\n",
    "# Commented example of using a loop for the filter.\n",
    "'''\n",
    "iso3_list = list(df_country_code['CountryCode'])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['geo'] not in iso3_list:\n",
    "        df.drop(index, inplace=True)\n",
    "'''\n",
    "\n",
    "\n",
    "df = df[df['geo'].isin(df_country_code['CountryCode'])]\n",
    "\n",
    "\n",
    "# This resets the index, something you often need to do when comparing filtered datasets with a common key.\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# we'll rename the column since it not excludes all but the country codes subset of the iso3 codes.\n",
    "df.rename(columns={'geo': 'country_code'}, inplace=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8f7465-07f5-4959-bc38-a3656ff505ee",
   "metadata": {},
   "source": [
    "# Posing questions\n",
    "\n",
    "Now at this point we could do a number of things, like translating the iso3 codes into country names. Or cleaning this data.\n",
    "However, given the nature of this data, cleaning like normal is going to ruin it. If we drop null columns we're going to\n",
    "Delete literally every value. Instead we want to just work with this data conditionally.\n",
    "\n",
    "We could begin by asking some very basic questions:\n",
    "    Q: What was the population in 1950 vs 2013?\n",
    "    Q: What about the top 10 largest countries in those two years?\n",
    "    \n",
    "Those sorts of questions are relatively 1-dimensional, though, and don't make for very interesting research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfc6d099-843c-4a68-b504-d1370ab32b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population 1950: 2,532,527,473\n",
      "Population 2013: 7,128,986,812\n"
     ]
    }
   ],
   "source": [
    "# population comparison of 1950 and 2013\n",
    "pop1950 = df[1950].sum()\n",
    "pop2013 = df[2013].sum()\n",
    "print(f'Population 1950: {pop1950:,.0f}')\n",
    "print(f'Population 2013: {pop2013:,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75df44cb-f51b-44da-910f-8047fa13e2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country_code        1950\n",
      "         CHN 550,800,000\n",
      "         IND 371,900,000\n",
      "         USA 157,800,000\n",
      "         RUS 102,700,000\n",
      "         JPN  82,200,000\n",
      "         IDN  74,840,000\n",
      "         DEU  68,380,000\n",
      "         BRA  53,970,000\n",
      "         GBR  50,620,000\n",
      "         ITA  46,370,000\n",
      "country_code          2013\n",
      "         CHN 1,359,000,000\n",
      "         IND 1,275,000,000\n",
      "         USA   318,500,000\n",
      "         IDN   247,200,000\n",
      "         BRA   200,100,000\n",
      "         PAK   183,200,000\n",
      "         NGA   170,900,000\n",
      "         BGD   154,400,000\n",
      "         RUS   142,600,000\n",
      "         JPN   126,300,000\n"
     ]
    }
   ],
   "source": [
    "# top ten largest countries in 1950 and 2013\n",
    "top10_1950 = df.nlargest(10, 1950)[['country_code', 1950]]\n",
    "top10_2013 = df.nlargest(10, 2013)[['country_code', 2013]]\n",
    "\n",
    "# Display the top 10 country codes\n",
    "print(top10_1950.to_string(index=False))\n",
    "print(top10_2013.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed3121-c490-444c-81b4-0c78fd9162ad",
   "metadata": {},
   "source": [
    "# Research Question\n",
    "\n",
    "Now let's pose an actual research question.\n",
    "\n",
    "Q: Is there a relationship between the increase or decline in a country's ranking and their gini coefficient?\n",
    "\n",
    "Put another way, have countries that have experienced lower relative growth in population experienced more or less inequality?\n",
    "\n",
    "\n",
    "For that let's add another dataset from gapminder and apply the same transformations we did earlier. Just as before we'll need to pivot it to wide format and do a little bit of formating to the column names and country codes as well as filter it with the country_code list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "022041b9-ae18-491b-85aa-4f599006178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and clean the second dataset\n",
    "df_country_gini = pd.read_csv('ddf--datapoints--gapminder_gini--by--geo--time.csv')\n",
    "df_country_gini = df_country_gini.pivot_table(index='geo', columns='time', values='gapminder_gini').reset_index()\n",
    "\n",
    "\n",
    "df_country_gini['geo'] = df_country_gini['geo'].str.upper()\n",
    "df_country_gini = df_country_gini[df_country_gini['geo'].isin(df_country_code['CountryCode'])]\n",
    "\n",
    "df_country_gini.reset_index(drop=True, inplace=True)\n",
    "df_country_gini.rename(columns={'geo': 'country_code'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474466d2-77f3-4248-a223-70961f5ccd82",
   "metadata": {},
   "source": [
    "# Filling in Data Gaps Note\n",
    "\n",
    "During my initial submission of this project the reviewer pointed out that large amounts of artificial data were generated in the following section. While that would normally be a valid concern for, example, business data or continuous data that is unpredictable, population data has these traits:\n",
    "\n",
    "1. It is predictable.\n",
    "2. It is indirectly measurable through qualitative means\n",
    "3. The primary source of it prior to the 1950's outside of the West is through these qualitative means.\n",
    "\n",
    "So running a regression to fill in an estimate of population between, for example, the year 1800 and 1820 doesn't produce invalid data or reduce the validity of the results--at least not below the standard of the social sciences. The trendlines are predictable, with large shifts in population being a relative rarity in history and population growth or decline happening at a glacial rate that's fairly consistent on a scale measured in centuries.\n",
    "\n",
    "Now why does this matter? There are lacunae in the estimates of population prior to the year 1900 for most countries, including in this gapminder dataset that we've used. Generally that's not an issue for the reasons I stated above--an historian or social scientist would simply draw a trend and use that number because in history there has almost never been an instance where populations shift up or down beyond the margin of error on a polity-scale measurement.\n",
    "\n",
    "\n",
    "That all being said, what are we doing below?\n",
    "\n",
    "# Interpolation of Historical Population Data By Year\n",
    "\n",
    "Below we start by comparing out gini and our population dataframes, after copying them, and then intersecting them so we only have common country_codes.\n",
    "\n",
    "We'll then check for any null values. As we can see, the gini information contains no null values while the population information contains many. However, if we do a check, we can see that the range of data that we have remaining, 1800-2013, has non-null values at both ends of the dataset. We know we have data in the lastest years for every country, so we only check for 1800, a year that stands out in demographic science for estimates of population.\n",
    "\n",
    "This makes a regression possible with a good level of certainty. Good enough for the resolution of our analysis, at least, which is not necessarily looking at any single country to draw conclusions.\n",
    "\n",
    "In an analysis where we were going to focus on a single country, we would probably want to pick an interval--such as 10 years--and try to find primary sources to corrobarate that there are no serious changes in the population that year such as migrations, wars, or famines. We would apply that analysis to any artificial data we generated. Then we would actually need to qualify these events' affect on the shape of the population curve. Having worked with demographic data extensively, it's uncommon for the effect to alter the trend. Eve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8bcdc970-95e7-475b-ab00-5686b4abb6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 194 entries, 0 to 193\n",
      "Columns: 215 entries, country_code to 2013\n",
      "dtypes: float64(214), object(1)\n",
      "memory usage: 326.0+ KB\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 194 entries, 0 to 193\n",
      "Columns: 215 entries, country_code to 2013\n",
      "dtypes: float64(214), object(1)\n",
      "memory usage: 326.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# first we'll do this so we don't destroy our original data\n",
    "dfpop = df\n",
    "dfgini = df_country_gini\n",
    "\n",
    "# ensure that we restrict our years to a common set\n",
    "common_columns = list(set(dfpop.columns) & set(dfgini.columns))\n",
    "common_country_codes = list(set(dfpop['country_code']) & set(dfgini['country_code']))\n",
    "\n",
    "# ensure that we restrict our country_codes to a common set\n",
    "dfpop = dfpop[dfpop['country_code'].isin(common_country_codes)][common_columns]\n",
    "dfgini = dfgini[dfgini['country_code'].isin(common_country_codes)][common_columns]\n",
    "\n",
    "# reset the indexes\n",
    "dfpop.reset_index(drop=True, inplace=True)\n",
    "dfgini.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dfpop.info()\n",
    "print()\n",
    "dfgini.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20e4ea97-40c6-4246-9cca-58a66a9287dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pop Nulls: True\n",
      "Gini Nulls: False\n",
      "1800 Has Nulls: False\n"
     ]
    }
   ],
   "source": [
    "# check for null values\n",
    "popnull = dfpop.isnull().values.any()\n",
    "gininull = dfgini.isnull().values.any()\n",
    "print(f'Pop Nulls: {popnull}')\n",
    "print(f'Gini Nulls: {gininull}')\n",
    "\n",
    "# check for null values in 1800\n",
    "nulls_1800 = dfpop[1800].isnull().any()\n",
    "print(f'1800 Has Nulls: {nulls_1800}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17669d4-78c7-4b13-a108-7fe662dbe4d8",
   "metadata": {},
   "source": [
    "# Algorithm To Fill Null Population Values\n",
    "\n",
    "This algorithm defines a function that can take a starting point and a stopping point within a row and a row itself. It then runs a linear regression to estimate the values between theose stopping points.\n",
    "\n",
    "That function is called by a loop which iterates through each row and then column identifying extents within the data that are filled by null values. The extents can be of arbitrary length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c428b12a-7682-4c3c-bb1a-b15e6c70a5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>time</th>\n",
       "      <th>country_code</th>\n",
       "      <th>1800</th>\n",
       "      <th>1801</th>\n",
       "      <th>1802</th>\n",
       "      <th>1803</th>\n",
       "      <th>1804</th>\n",
       "      <th>1805</th>\n",
       "      <th>1806</th>\n",
       "      <th>1807</th>\n",
       "      <th>1808</th>\n",
       "      <th>...</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>3,280,000</td>\n",
       "      <td>3,280,000</td>\n",
       "      <td>3,280,000</td>\n",
       "      <td>3,280,000</td>\n",
       "      <td>3,280,000</td>\n",
       "      <td>3,280,000</td>\n",
       "      <td>3,280,000</td>\n",
       "      <td>3,280,000</td>\n",
       "      <td>3,280,000</td>\n",
       "      <td>...</td>\n",
       "      <td>26,690,000</td>\n",
       "      <td>27,610,000</td>\n",
       "      <td>28,420,000</td>\n",
       "      <td>29,150,000</td>\n",
       "      <td>29,840,000</td>\n",
       "      <td>30,580,000</td>\n",
       "      <td>31,410,000</td>\n",
       "      <td>32,360,000</td>\n",
       "      <td>33,400,000</td>\n",
       "      <td>34,500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGO</td>\n",
       "      <td>1,567,000</td>\n",
       "      <td>1,567,000</td>\n",
       "      <td>1,567,000</td>\n",
       "      <td>1,567,000</td>\n",
       "      <td>1,567,000</td>\n",
       "      <td>1,567,000</td>\n",
       "      <td>1,567,000</td>\n",
       "      <td>1,567,000</td>\n",
       "      <td>1,567,000</td>\n",
       "      <td>...</td>\n",
       "      <td>15,960,000</td>\n",
       "      <td>16,490,000</td>\n",
       "      <td>17,010,000</td>\n",
       "      <td>17,530,000</td>\n",
       "      <td>18,040,000</td>\n",
       "      <td>18,560,000</td>\n",
       "      <td>19,080,000</td>\n",
       "      <td>19,620,000</td>\n",
       "      <td>20,160,000</td>\n",
       "      <td>20,710,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALB</td>\n",
       "      <td>410,400</td>\n",
       "      <td>411,667</td>\n",
       "      <td>412,933</td>\n",
       "      <td>414,200</td>\n",
       "      <td>415,467</td>\n",
       "      <td>416,733</td>\n",
       "      <td>418,000</td>\n",
       "      <td>419,267</td>\n",
       "      <td>420,533</td>\n",
       "      <td>...</td>\n",
       "      <td>3,125,000</td>\n",
       "      <td>3,142,000</td>\n",
       "      <td>3,157,000</td>\n",
       "      <td>3,170,000</td>\n",
       "      <td>3,181,000</td>\n",
       "      <td>3,193,000</td>\n",
       "      <td>3,204,000</td>\n",
       "      <td>3,216,000</td>\n",
       "      <td>3,227,000</td>\n",
       "      <td>3,238,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AND</td>\n",
       "      <td>2,654</td>\n",
       "      <td>2,654</td>\n",
       "      <td>2,654</td>\n",
       "      <td>2,654</td>\n",
       "      <td>2,654</td>\n",
       "      <td>2,654</td>\n",
       "      <td>2,654</td>\n",
       "      <td>2,654</td>\n",
       "      <td>2,654</td>\n",
       "      <td>...</td>\n",
       "      <td>75,290</td>\n",
       "      <td>77,890</td>\n",
       "      <td>79,870</td>\n",
       "      <td>81,390</td>\n",
       "      <td>82,580</td>\n",
       "      <td>83,680</td>\n",
       "      <td>84,860</td>\n",
       "      <td>86,160</td>\n",
       "      <td>87,520</td>\n",
       "      <td>88,910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARE</td>\n",
       "      <td>40,150</td>\n",
       "      <td>40,150</td>\n",
       "      <td>40,150</td>\n",
       "      <td>40,150</td>\n",
       "      <td>40,150</td>\n",
       "      <td>40,150</td>\n",
       "      <td>40,150</td>\n",
       "      <td>40,150</td>\n",
       "      <td>40,150</td>\n",
       "      <td>...</td>\n",
       "      <td>3,658,000</td>\n",
       "      <td>4,069,000</td>\n",
       "      <td>4,663,000</td>\n",
       "      <td>5,406,000</td>\n",
       "      <td>6,207,000</td>\n",
       "      <td>6,939,000</td>\n",
       "      <td>7,512,000</td>\n",
       "      <td>7,891,000</td>\n",
       "      <td>8,106,000</td>\n",
       "      <td>8,208,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ARG</td>\n",
       "      <td>534,000</td>\n",
       "      <td>534,000</td>\n",
       "      <td>534,000</td>\n",
       "      <td>534,000</td>\n",
       "      <td>534,000</td>\n",
       "      <td>534,000</td>\n",
       "      <td>534,000</td>\n",
       "      <td>534,000</td>\n",
       "      <td>534,000</td>\n",
       "      <td>...</td>\n",
       "      <td>38,340,000</td>\n",
       "      <td>38,680,000</td>\n",
       "      <td>39,020,000</td>\n",
       "      <td>39,370,000</td>\n",
       "      <td>39,710,000</td>\n",
       "      <td>40,060,000</td>\n",
       "      <td>40,410,000</td>\n",
       "      <td>40,760,000</td>\n",
       "      <td>41,120,000</td>\n",
       "      <td>41,470,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ARM</td>\n",
       "      <td>413,300</td>\n",
       "      <td>413,300</td>\n",
       "      <td>413,300</td>\n",
       "      <td>413,300</td>\n",
       "      <td>413,300</td>\n",
       "      <td>413,300</td>\n",
       "      <td>413,300</td>\n",
       "      <td>413,300</td>\n",
       "      <td>413,300</td>\n",
       "      <td>...</td>\n",
       "      <td>3,063,000</td>\n",
       "      <td>3,066,000</td>\n",
       "      <td>3,070,000</td>\n",
       "      <td>3,074,000</td>\n",
       "      <td>3,079,000</td>\n",
       "      <td>3,085,000</td>\n",
       "      <td>3,092,000</td>\n",
       "      <td>3,100,000</td>\n",
       "      <td>3,109,000</td>\n",
       "      <td>3,118,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ATG</td>\n",
       "      <td>37,000</td>\n",
       "      <td>37,000</td>\n",
       "      <td>37,000</td>\n",
       "      <td>37,000</td>\n",
       "      <td>37,000</td>\n",
       "      <td>37,000</td>\n",
       "      <td>37,000</td>\n",
       "      <td>37,000</td>\n",
       "      <td>37,000</td>\n",
       "      <td>...</td>\n",
       "      <td>82,840</td>\n",
       "      <td>83,920</td>\n",
       "      <td>84,950</td>\n",
       "      <td>85,940</td>\n",
       "      <td>86,880</td>\n",
       "      <td>87,800</td>\n",
       "      <td>88,710</td>\n",
       "      <td>89,610</td>\n",
       "      <td>90,510</td>\n",
       "      <td>91,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AUS</td>\n",
       "      <td>351,000</td>\n",
       "      <td>350,190</td>\n",
       "      <td>349,381</td>\n",
       "      <td>348,571</td>\n",
       "      <td>347,762</td>\n",
       "      <td>346,952</td>\n",
       "      <td>346,143</td>\n",
       "      <td>345,333</td>\n",
       "      <td>344,524</td>\n",
       "      <td>...</td>\n",
       "      <td>20,100,000</td>\n",
       "      <td>20,400,000</td>\n",
       "      <td>20,740,000</td>\n",
       "      <td>21,120,000</td>\n",
       "      <td>21,510,000</td>\n",
       "      <td>21,900,000</td>\n",
       "      <td>22,270,000</td>\n",
       "      <td>22,610,000</td>\n",
       "      <td>22,920,000</td>\n",
       "      <td>23,210,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AUT</td>\n",
       "      <td>3,206,000</td>\n",
       "      <td>3,213,762</td>\n",
       "      <td>3,221,524</td>\n",
       "      <td>3,229,286</td>\n",
       "      <td>3,237,048</td>\n",
       "      <td>3,244,810</td>\n",
       "      <td>3,252,571</td>\n",
       "      <td>3,260,333</td>\n",
       "      <td>3,268,095</td>\n",
       "      <td>...</td>\n",
       "      <td>8,186,000</td>\n",
       "      <td>8,232,000</td>\n",
       "      <td>8,273,000</td>\n",
       "      <td>8,310,000</td>\n",
       "      <td>8,342,000</td>\n",
       "      <td>8,370,000</td>\n",
       "      <td>8,394,000</td>\n",
       "      <td>8,413,000</td>\n",
       "      <td>8,429,000</td>\n",
       "      <td>8,441,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 215 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "time country_code      1800      1801      1802      1803      1804      1805  \\\n",
       "0             AFG 3,280,000 3,280,000 3,280,000 3,280,000 3,280,000 3,280,000   \n",
       "1             AGO 1,567,000 1,567,000 1,567,000 1,567,000 1,567,000 1,567,000   \n",
       "2             ALB   410,400   411,667   412,933   414,200   415,467   416,733   \n",
       "3             AND     2,654     2,654     2,654     2,654     2,654     2,654   \n",
       "4             ARE    40,150    40,150    40,150    40,150    40,150    40,150   \n",
       "5             ARG   534,000   534,000   534,000   534,000   534,000   534,000   \n",
       "6             ARM   413,300   413,300   413,300   413,300   413,300   413,300   \n",
       "7             ATG    37,000    37,000    37,000    37,000    37,000    37,000   \n",
       "8             AUS   351,000   350,190   349,381   348,571   347,762   346,952   \n",
       "9             AUT 3,206,000 3,213,762 3,221,524 3,229,286 3,237,048 3,244,810   \n",
       "\n",
       "time      1806      1807      1808  ...       2004       2005       2006  \\\n",
       "0    3,280,000 3,280,000 3,280,000  ... 26,690,000 27,610,000 28,420,000   \n",
       "1    1,567,000 1,567,000 1,567,000  ... 15,960,000 16,490,000 17,010,000   \n",
       "2      418,000   419,267   420,533  ...  3,125,000  3,142,000  3,157,000   \n",
       "3        2,654     2,654     2,654  ...     75,290     77,890     79,870   \n",
       "4       40,150    40,150    40,150  ...  3,658,000  4,069,000  4,663,000   \n",
       "5      534,000   534,000   534,000  ... 38,340,000 38,680,000 39,020,000   \n",
       "6      413,300   413,300   413,300  ...  3,063,000  3,066,000  3,070,000   \n",
       "7       37,000    37,000    37,000  ...     82,840     83,920     84,950   \n",
       "8      346,143   345,333   344,524  ... 20,100,000 20,400,000 20,740,000   \n",
       "9    3,252,571 3,260,333 3,268,095  ...  8,186,000  8,232,000  8,273,000   \n",
       "\n",
       "time       2007       2008       2009       2010       2011       2012  \\\n",
       "0    29,150,000 29,840,000 30,580,000 31,410,000 32,360,000 33,400,000   \n",
       "1    17,530,000 18,040,000 18,560,000 19,080,000 19,620,000 20,160,000   \n",
       "2     3,170,000  3,181,000  3,193,000  3,204,000  3,216,000  3,227,000   \n",
       "3        81,390     82,580     83,680     84,860     86,160     87,520   \n",
       "4     5,406,000  6,207,000  6,939,000  7,512,000  7,891,000  8,106,000   \n",
       "5    39,370,000 39,710,000 40,060,000 40,410,000 40,760,000 41,120,000   \n",
       "6     3,074,000  3,079,000  3,085,000  3,092,000  3,100,000  3,109,000   \n",
       "7        85,940     86,880     87,800     88,710     89,610     90,510   \n",
       "8    21,120,000 21,510,000 21,900,000 22,270,000 22,610,000 22,920,000   \n",
       "9     8,310,000  8,342,000  8,370,000  8,394,000  8,413,000  8,429,000   \n",
       "\n",
       "time       2013  \n",
       "0    34,500,000  \n",
       "1    20,710,000  \n",
       "2     3,238,000  \n",
       "3        88,910  \n",
       "4     8,208,000  \n",
       "5    41,470,000  \n",
       "6     3,118,000  \n",
       "7        91,400  \n",
       "8    23,210,000  \n",
       "9     8,441,000  \n",
       "\n",
       "[10 rows x 215 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "def nullFill(start, stop, row):\n",
    "    # first we break up our input values, which we passed as a tuple.\n",
    "    start_value, start_i = start\n",
    "    end_value, end_i = stop\n",
    "    # get our list of null years\n",
    "    null_years = row.index[start_i:end_i]\n",
    "    # now we run our regression, grabbing the x and y for the model from our starting values.\n",
    "    x = np.array([start_i - 1, end_i + 1])\n",
    "    y = np.array([start_value, end_value])\n",
    "    coefficients = np.polyfit(x, y, deg=1)\n",
    "    null_values = np.polyval(coefficients, np.arange(start_i, end_i))\n",
    "    \n",
    "    row[null_years] = null_values\n",
    "    return row\n",
    "\n",
    "\n",
    "# now let's go through out dataset finding null extents and fill ing them.\n",
    "for index, row in dfpop.iterrows():\n",
    "    for i, value in enumerate(row):\n",
    "        # if the value is null but the previous value isn't, we've found a null extent.\n",
    "        if pd.isnull(value) and not pd.isnull(row.values[i - 1]):\n",
    "            nullExStart = (row.values[i - 1], i)\n",
    "        # find the end of the null extent\n",
    "        if not pd.isnull(value) and pd.isnull(row.values[i - 1]):\n",
    "            nullExEnd = (row.values[i], i)\n",
    "            # now that we have our end, we can handle the extent\n",
    "            dfpop.loc[index] = nullFill(nullExStart, nullExEnd, row)\n",
    "\n",
    "dfpop.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cbf901-3f83-441b-b4f6-db44ac995a75",
   "metadata": {},
   "source": [
    "# Testing Setup\n",
    "\n",
    "Now we have what should be two nice and complete little datasets with reasonably interpolated synthetic datapoints in place of the null values.\n",
    "\n",
    "Now we can see if there's any correlation between inequality and a country's calculated rank in a given year.\n",
    "The below cell will prompt you for valid input numbers within the range of our dataset. It's designed to work with a dataset of any size so long as it's mapped the same.\n",
    "\n",
    "The cell after that will then grab the start and end ranks for the countries, calculating how many ranks they have changed. We haven't set this up to display country_code because we aren't interested in the performance of individual countries, but doing so would be relatively easy, mapping the index to the source dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad26301d-2f09-4111-bcd5-2bcf98754097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the start year:  1800\n",
      "Enter the end year:  2013\n"
     ]
    }
   ],
   "source": [
    "# hint: enter numbers between 1800 and 2013\n",
    "while True:\n",
    "    start_year = int(input(\"Enter the start year: \"))\n",
    "    end_year = int(input(\"Enter the end year: \"))\n",
    "    if start_year < end_year and start_year >= dfpop.columns[1] and end_year <= dfpop.columns[-1]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5d174fc-2fea-4ecf-b6f3-c2505a7cd2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      34\n",
      "1      62\n",
      "2     114\n",
      "3     191\n",
      "4     168\n",
      "       ..\n",
      "189   166\n",
      "190    40\n",
      "191    64\n",
      "192    94\n",
      "193    75\n",
      "Name: 1800, Length: 194, dtype: float64\n",
      "0       5\n",
      "1      -4\n",
      "2      19\n",
      "3      -7\n",
      "4     -74\n",
      "       ..\n",
      "189     9\n",
      "190     7\n",
      "191   -40\n",
      "192   -25\n",
      "193    -4\n",
      "Length: 194, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# this gives us countries ranked by population\n",
    "start_rank = dfpop[start_year].rank(ascending=False, method='min')\n",
    "print(start_rank)\n",
    "\n",
    "# This grabs us the change in a country's rank in population.\n",
    "change_in_rank = dfpop[end_year].rank(ascending=False, method='min') - start_rank\n",
    "print(change_in_rank)\n",
    "\n",
    "# this grabs the mean gini coefficient for the time period entered\n",
    "gini_columns = list(range(start_year, end_year))\n",
    "gini_coefficients = dfgini[gini_columns].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b028c3-10d9-41d4-8df6-018aace95836",
   "metadata": {},
   "source": [
    "# Correlation Testing\n",
    "\n",
    "Below we'll generate some correlation coefficients, which we'll interpret in our conclusion. These use the numpy function corrcoef, which takes two values and compares them at each index, generating a score between -1 and 1 for how they are correlated. This function is a Pearson analysis by default. Non-0 values indicate a relationship.\n",
    "\n",
    "Will also print the Fisher z-score and related stats of our main research question correlation score, which is listed below as:\n",
    "> Correlation between change in rank and Gini coefficient\n",
    "\n",
    "Then generate a z-test from that to see if our results are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1784689b-3b67-403f-83ce-8b7b804bb239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between starting rank and change in rank: -0.23758802477608323\n",
      "Correlation between starting rank and Gini coefficient: 0.16377049446028172\n",
      "Correlation between change in rank and Gini coefficient: -0.3697287095169644\n"
     ]
    }
   ],
   "source": [
    "# here we calculate and print our correlation coefficients\n",
    "correlation_start_rank_change = np.corrcoef(start_rank, change_in_rank)[0, 1]\n",
    "correlation_start_rank_gini = np.corrcoef(start_rank, gini_coefficients)[0, 1]\n",
    "correlation_change_in_rank_gini = np.corrcoef(change_in_rank, gini_coefficients)[0, 1]\n",
    "\n",
    "print(\"Correlation between starting rank and change in rank:\", correlation_start_rank_change)\n",
    "print(\"Correlation between starting rank and Gini coefficient:\", correlation_start_rank_gini)\n",
    "print(\"Correlation between change in rank and Gini coefficient:\", correlation_change_in_rank_gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5dd2759-db00-48b8-99a8-cfcea9acc97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 194 entries, 0 to 193\n",
      "Columns: 215 entries, country_code to 2013\n",
      "dtypes: float64(214), object(1)\n",
      "memory usage: 326.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# here we create a summary for our main research question, which is the \n",
    "r = correlation_change_in_rank_gini\n",
    "# grabs the sample size, minus the header\n",
    "N = dfpop.shape[0] - 1\n",
    "\n",
    "# Fisher z-score calculation\n",
    "z = 0.5 * np.log((1 + r) / (1 - r))\n",
    "\n",
    "# standard error\n",
    "std_err = 1 / np.sqrt(N - 3)\n",
    "\n",
    "# z-test calculation\n",
    "zt = z / std_err\n",
    "\n",
    "# p-value, finally\n",
    "p = 2 * (1- norm.cdf(np.abs(z_test)))\n",
    "\n",
    "print(f'P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3ee15-89de-4a67-b8c9-5854ef293b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we could move on to other angles of analysis from here to understand these relationships. For example:\n",
    "# Making a regression showing the relationship between start and end rank.\n",
    "sbn.regplot(x=start_rank, y=change_in_rank)\n",
    "mpl.xlabel(f'Starting Rank ({start_year}-{end_year})')\n",
    "mpl.ylabel(\"Change in Rank\")\n",
    "mpl.title(f'Regression Analysis: Change in Rank vs Starting Rank ({start_year}-{end_year})')\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6274ed-f9a7-4f8d-8e63-a9b9b972a349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or start rank and gini\n",
    "sbn.regplot(x=start_rank, y=gini_coefficients)\n",
    "\n",
    "mpl.xlabel(f'Population Rank ({start_year}-{end_year})')\n",
    "mpl.ylabel(\"Gini Coefficient\")\n",
    "mpl.title(f'Regression Analysis: Gini Coefficient vs Population Rank ({start_year}-{end_year})')\n",
    "mpl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ce511-4583-44d9-ba8f-a2759aba2f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# or change in rank vs gini\n",
    "sbn.regplot(x=change_in_rank, y=gini_coefficients)\n",
    "\n",
    "mpl.xlabel(f'Change in Rank ({start_year}-{end_year})')\n",
    "mpl.ylabel(\"Gini Coefficient\")\n",
    "mpl.title(f'Regression Analysis: Gini Coefficient vs Change in Rank ({start_year}-{end_year})')\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230dd854-b915-4c9f-a6f0-55f26ba2db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alright we have some results!\n",
    "# This will be the report sections!\n",
    "'''\n",
    "    Research Question:\n",
    "        Is there a correlation between a country's relative population size to other countries and its level of inequality?\n",
    "        For this writeup we'll test 1800 to 2013.\n",
    "    \n",
    "    Results:\n",
    "        How should we interpret these results?\n",
    "            Remember that negative numbers indicate a negative relationship, 0 no relationship, and positive a positive relationship.\n",
    "            Positive: as one thing increases the latter thing increases (rank change up, gini up)\n",
    "            Zero:     no relatinship\n",
    "            Negatie:  as one thing increases the latter thing decreases (rank change up, gini down)\n",
    "\n",
    "        Let's go over them, then.\n",
    "            * Correlation between starting rank and change in rank: -0.23758802477608323\n",
    "                This indicates that higher starting ranks correlated with bigger drops in rank and vice versa.\n",
    "\n",
    "            * Correlation between starting rank and Gini coefficient: 0.16377049446028172\n",
    "                This indicated that bigger countries in 1800 tended to have more inequality going forward.\n",
    "\n",
    "            * Correlation between change in rank and Gini coefficient: -0.3697287095169644\n",
    "                This indicated that countries with higher positive changes in rank experienced lower inequality.\n",
    "    \n",
    "    Discussion:\n",
    "        There are likely a lot of reasons why our results look the way they do. First off, inequality is not a perfect\n",
    "        measurement of economic success. Poor countries tend to have low inequality where rich countries tend to have\n",
    "        a much higher level of it. However, the poor in rich countries are typically better off than \n",
    "        even the rich in poor countries.\n",
    "        \n",
    "        I tested three hypotheses here, and found clear patterns in each. Likewise, if you restrict the timescale to \n",
    "        1900-2013 the correlations actually grow as we have less flat-synthetic data in the gini dataset.\n",
    "        Overall the \n",
    "        \n",
    "    Further efforts:\n",
    "        At this point the next research step would be to run many correlations and perform an analysis of statistical\n",
    "        power in r-studio or something else.\n",
    "        \n",
    "        More graphs could also be produced, but the nature of my research question here is not condusive to those being useful.\n",
    "        From here the natural steps would be to break the data down by region (easily done given I used iso3 codes) and do\n",
    "        separate analyses of regions, cultures, etc. Adding additional data onto these sets is very easily done.\n",
    "        \n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
